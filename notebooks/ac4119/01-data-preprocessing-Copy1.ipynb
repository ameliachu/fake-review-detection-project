{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../../data/raw/train.csv'\n",
    "val_path = '../../data/raw/dev.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(val_path)\n",
    "\n",
    "base = '/Users/chuamelia/Google Drive/Spring 2020/Machine Learning/fake-review-detection-project/data/processed/dev/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset\n",
    "**Methodology:**\n",
    "\n",
    "1. Separate out the negative examples (dominant class)\n",
    "2. Determine the number of dataframes (`num_splits`) needed to incorporate all negative examples.\n",
    "3. Create a list of dataframes containing the different splits of negative examples.\n",
    "4. Concat the positive and negative examples back together.\n",
    "    - For each new training set, include a 80% random sample of the positive examples to aviod overfitting to the\n",
    "    postive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting frac = 1 to shuffle all the data\n",
    "full_negative_examples = train[train['label']==0].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining the number of positive and negative examples \n",
    "# to determine the number of splits  \n",
    "positive_examples = train[train['label']==1]\n",
    "num_pos_examples = positive_examples.count()[0]\n",
    "num_neg_examples = full_negative_examples.count()[0]\n",
    "\n",
    "num_splits = int(round(num_neg_examples / num_pos_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_train_data = [full_negative_examples[ i * num_pos_examples : min((i + 1) * num_pos_examples, num_neg_examples)] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sets = []\n",
    "for i, negative_examples in enumerate(neg_train_data):\n",
    "    train_set_fname = 'ac4119_train_set_{0}.csv'.format(i)\n",
    "    positive_examples = train[train['label']==1].sample(frac=.8)\n",
    "    # Unioning the positive and negative examples \n",
    "    # Then shuffling so that not all negative examples are at the end\n",
    "    train_set = pd.concat([negative_examples, positive_examples], ignore_index=True).sample(frac=1)\n",
    "    training_sets.append(train_set)\n",
    "    #train_set.to_csv(base + train_set_fname, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ac4119_train_set_8'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_fname "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 training sets.\n"
     ]
    }
   ],
   "source": [
    "return_text = \"There are {0} training sets.\".format(len(training_sets))\n",
    "print(return_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_obj(fname):\n",
    "    directory = '../../data/processed/dev/'\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(directory + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# load_obj(word_freq , 'word_freq_ac4119')\n",
    "word_freq_real = load_obj('word_freq_real_ac4119')\n",
    "word_freq_fake = load_obj('word_freq_fake_ac4119')\n",
    "\n",
    "sorted_freq_fake = {k: v for k, v in sorted(word_freq_fake.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_freq_real = {k: v for k, v in sorted(word_freq_real.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "top_fake_words = list(sorted_freq_fake.keys())[:100]\n",
    "top_real_words = list(sorted_freq_real.keys())[:100]\n",
    "\n",
    "top_overlapping_words = list(set(top_fake_words).intersection(set(top_real_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "# https://www.python.org/dev/peps/pep-0289/\n",
    "def rm_stop_words(tokens):\n",
    "    filtered = [w for w in tokens if not w.lower() in stop_words] \n",
    "    return filtered\n",
    "\n",
    "def rm_puctuation(tokens):\n",
    "    init_filter = [w for w in tokens if w not in string.punctuation]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in init_filter]\n",
    "    clean = [w for w in stripped if w not in ['',' ']]\n",
    "    return clean\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    filtered = rm_stop_words(tokens)\n",
    "    new_tokens = rm_puctuation(filtered)\n",
    "    return new_tokens\n",
    "\n",
    "def spellcheck_tokens(tokens):\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    cleaned_tokens = [spell.correction(word) if word in misspelled  else word for word in tokens ]\n",
    "    return cleaned_tokens\n",
    "\n",
    "def rm_overlap_words(tokens):\n",
    "    filtered = [token for token in tokens if token not in top_overlapping_words]\n",
    "    return filtered\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    stg_tokens_1 = rm_stop_words(tokens)\n",
    "    stg_tokens_2 = rm_puctuation(stg_tokens_1)\n",
    "    stg_tokens_3 = spellcheck_tokens(stg_tokens_2)\n",
    "    new_tokens = rm_overlap_words(stg_tokens_3)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set['token_review'] = train_set['review'].map(lambda x: clean_tokens(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(base + '/ac4119_train_set_8_w_tokens_nltk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['SELF', 'RESPECTING', 'WOMAN', 'LOOKING', 'EAT', 'DELICIOUS', 'FOOD', 'RESERVE', 'TABLE', 'BIRTHDAY', 'PARTY', 'LOOK', 'ELSEWHERE', 'PLEASE', 'reservation', 'birthday', 'several', 'days', 'actual', 'day', 'offered', 'different', 'times', 'apart', 'mean', 'interval', 'ridiculous', 'Something', '645', '1130', 'opted', 'earlier', 'though', 'worked', '7', 'Upon', 'arriving', 'took', 'seat', 'kindly', 'offered', 'cocktail', 'AMAZINGLY', 'helpful', 'welcoming', 'bartenders', 'taken', 'card', 'put', 'file', 'reserve', 'told', 'mine', 'hours', 'waited', 'arrive', 'MANAGER', 'told', 'gorgeous', 'need', 'personally', 'take', 'dress', 'since', 'flowing', 'shirt', 'dress', 'belt', 'dress', 'justice', 'exclaimed', 'EXCUSE', 'proceeded', 'add', 'needed', 'show', 'leg', 'figure', 'dress', 'unflattering', 'CLEARLY', 'RING', 'FINGER', 'MARRIED', 'MAN', 'interjected', 'saw', 'ring', 'simply', 'shrugged', 'told', 'mad', 'flirt', '0', 'arrived', '730', 'maitre', 'informed', 'looked', 'told', 'sat', 'others', 'TABLE', 'RESERVED', '2', 'HOURS', 'continued', 'add', 'walking', 'take', 'precedence', 'completely', 'baffles', 'JEEZ', 'point', 'making', 'reservation', 'MEANT', 'RESERVE', 'SPOT', 'ALLOTTED', 'PERIOD', 'TIME', 'Unfortunately', 'massively', 'uncomfortable', 'instances', 'enough', 'ruin', 'perception', 'drinks', 'POOR', 'POOR', 'POOR', 'POOR', 'POOR', 'PUTRID', 'SERVICE', 'DISGUSTED', 'owner', 'firing', 'entire', 'management', 'Like', 'said', 'quantity', 'joke', 'pricing', 'menial', 'meals', 'surprising', 'thing', 'expressed', 'annoyance', 'offered', 'apology', 'Simple', 'return', 'appreciate', 'honesty', 'd', 'believe', 'review', 'give', 'credit', 'due'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[train_set['ex_id']==73795]['token_review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_obj(fname):\n",
    "    directory = '../../data/processed/dev/'\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(directory + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Out of the top 100 words for both real and fake reviews {} overlapped\".format(len(top_overlapping_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_tokenizer(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cnt_vectorizer = CountVectorizer( tokenizer=identity_tokenizer,  stop_words='english', lowercase=False, binary=True)\n",
    "tfidf_vectorizer = TfidfVectorizer( tokenizer=identity_tokenizer, stop_words='english', lowercase=False, binary=True, min_df=5)\n",
    "cnt_vectorizer.fit(train_set['token_review'])\n",
    "tfidf_vectorizer.fit(train_set['token_review'])\n",
    "\n",
    "cnt_X_train = cnt_vectorizer.transform(train_set['token_review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train_set['token_review'])\n",
    "\n",
    "# cnt_X_dev = cnt_vectorizer.transform(validation['review'])\n",
    "# tfidf_X_dev = tfidf_vectorizer.transform(validation['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "params = {'solver':'liblinear', 'max_iter':1000, 'class_weight': 'balanced', 'random_state': 519}\n",
    "tfidf_lr = LogisticRegression(**params )\n",
    "# tfidf_lr = LogisticRegression()\n",
    "fitted_tfidf_lr = tfidf_lr.fit(tfidf_X_train, Y_train)\n",
    "# fitted_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7455181572092549"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_tfidf_lr.score(tfidf_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7534348025946167"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', binary=True)\n",
    "tfidf_vectorizer.fit(train_set['review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train_set['review'])\n",
    "tfidf_lr = LogisticRegression(**params )\n",
    "fitted_tfidf_lr = tfidf_lr.fit(tfidf_X_train, Y_train)\n",
    "fitted_tfidf_lr.score(tfidf_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7398998927422238"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', binary=True, min_df=5)\n",
    "tfidf_vectorizer.fit(train_set['review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train_set['review'])\n",
    "tfidf_lr = LogisticRegression(**params )\n",
    "fitted_tfidf_lr = tfidf_lr.fit(tfidf_X_train, Y_train)\n",
    "fitted_tfidf_lr.score(tfidf_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
