{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/raw/train.csv'\n",
    "val_path = '../../data/raw/dev.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(val_path)\n",
    "\n",
    "base = '/Users/chuamelia/Google Drive/Spring 2020/Machine Learning/fake-review-detection-project/data/processed/dev/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset\n",
    "**Methodology:**\n",
    "\n",
    "1. Separate out the negative examples (dominant class)\n",
    "2. Determine the number of dataframes (`num_splits`) needed to incorporate all negative examples.\n",
    "3. Create a list of dataframes containing the different splits of negative examples.\n",
    "4. Concat the positive and negative examples back together.\n",
    "    - For each new training set, include a 80% random sample of the positive examples to aviod overfitting to the\n",
    "    postive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting frac = 1 to shuffle all the data\n",
    "full_negative_examples = train[train['label']==0].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the number of positive and negative examples \n",
    "# to determine the number of splits  \n",
    "positive_examples = train[train['label']==1]\n",
    "num_pos_examples = positive_examples.count()[0]\n",
    "num_neg_examples = full_negative_examples.count()[0]\n",
    "\n",
    "num_splits = int(round(num_neg_examples / num_pos_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_data = [full_negative_examples[ i * num_pos_examples : min((i + 1) * num_pos_examples, num_neg_examples)] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sets = []\n",
    "for i, negative_examples in enumerate(neg_train_data):\n",
    "    train_set_fname = 'ac4119_train_set_{0}'.format(i)\n",
    "    positive_examples = train[train['label']==1].sample(frac=.8)\n",
    "    # Unioning the positive and negative examples \n",
    "    # Then shuffling so that not all negative examples are at the end\n",
    "    train_set = pd.concat([negative_examples, positive_examples], ignore_index=True).sample(frac=1)\n",
    "    training_sets.append(train_set)\n",
    "    train_set.to_csv(base + train_set_fname, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 training sets.\n"
     ]
    }
   ],
   "source": [
    "return_text = \"There are {0} training sets.\".format(len(training_sets))\n",
    "print(return_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def rm_stop_words(tokens):\n",
    "    filtered = [w for w in tokens if not w.lower() in stop_words] \n",
    "    return filtered\n",
    "\n",
    "def rm_puctuation(tokens):\n",
    "    init_filter = [w for w in tokens if w not in string.punctuation]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in init_filter]\n",
    "    clean = [w for w in stripped if w not in ['',' ']]\n",
    "    return clean\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    filtered = rm_stop_words(tokens)\n",
    "    new_tokens = rm_puctuation(filtered)\n",
    "    return new_tokens\n",
    "\n",
    "def spellcheck_tokens(tokens):\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    cleaned_tokens = [spell.correction(word) if word in misspelled  else word for word in tokens ]\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typing this place into Yelp's search bar, I accidenlly typed \"Brooklyn Crap\" and thought to myself, how appropos.     It's a shame really. Brooklyn Crap has an amazing location and is really, really fun. Mini golf and other games and time wasters on the first floor really do make for a good experience. The problem is the food stinks, it's way overpriced and the wait staff seem like total novices.  The wait times are not only atrocious, but inaccurate.     It's such a shame - this place could've been one of the greats. Instead, it's one of the worst.\n"
     ]
    }
   ],
   "source": [
    "# Sample text used to benchmark both Spell Checkers.\n",
    "sample_text = \"\"\"Typing this place into Yelp's search bar, I accidenlly typed \"Brooklyn Crap\" and thought to myself, how appropos.     It's a shame really. Brooklyn Crap has an amazing location and is really, really fun. Mini golf and other games and time wasters on the first floor really do make for a good experience. The problem is the food stinks, it's way overpriced and the wait staff seem like total novices.  The wait times are not only atrocious, but inaccurate.     It's such a shame - this place could've been one of the greats. Instead, it's one of the worst.\"\"\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Typing', 'place', 'Yelp', 's', 'search', 'bar', 'accidently', 'typed', 'Brooklyn', 'Crap', 'thought', 'apropos', 's', 'shame', 'really', 'Brooklyn', 'Crap', 'amazing', 'location', 'really', 'really', 'fun', 'Mini', 'golf', 'games', 'time', 'wasters', 'first', 'floor', 'really', 'make', 'good', 'experience', 'problem', 'food', 'stinks', 's', 'way', 'overpriced', 'wait', 'staff', 'seem', 'like', 'total', 'novices', 'wait', 'times', 'atrocious', 'inaccurate', 's', 'shame', 'place', 'could', 've', 'one', 'greats', 'Instead', 's', 'one', 'worst']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sample_text)\n",
    "filtered_tokens = filter_tokens(tokens)\n",
    "print(spellcheck_tokens(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
