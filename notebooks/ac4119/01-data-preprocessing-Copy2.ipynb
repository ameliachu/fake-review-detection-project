{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../../data/raw/train.csv'\n",
    "val_path = '../../data/raw/dev.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(val_path)\n",
    "\n",
    "base = '/Users/chuamelia/Google Drive/Spring 2020/Machine Learning/fake-review-detection-project/data/processed/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32270</td>\n",
       "      <td>32270</td>\n",
       "      <td>32270</td>\n",
       "      <td>32270</td>\n",
       "      <td>32270</td>\n",
       "      <td>32270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ex_id  user_id  prod_id  rating   date  review\n",
       "label                                                \n",
       "0      32270    32270    32270   32270  32270   32270\n",
       "1       3648     3648     3648    3648   3648    3648"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35918"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3648+32270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset\n",
    "**Methodology:**\n",
    "\n",
    "1. Separate out the negative examples (dominant class)\n",
    "2. Determine the number of dataframes (`num_splits`) needed to incorporate all negative examples.\n",
    "3. Create a list of dataframes containing the different splits of negative examples.\n",
    "4. Concat the positive and negative examples back together.\n",
    "    - For each new training set, include a 80% random sample of the positive examples to aviod overfitting to the\n",
    "    postive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting frac = 1 to shuffle all the data\n",
    "full_negative_examples = train[train['label']==0].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining the number of positive and negative examples \n",
    "# to determine the number of splits  \n",
    "positive_examples = train[train['label']==1]\n",
    "num_pos_examples = positive_examples.count()[0]\n",
    "num_neg_examples = full_negative_examples.count()[0]\n",
    "\n",
    "num_splits = int(round(num_neg_examples / num_pos_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_train_data = [full_negative_examples[ i * num_pos_examples : min((i + 1) * num_pos_examples, num_neg_examples)] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sets = []\n",
    "for i, negative_examples in enumerate(neg_train_data):\n",
    "    train_set_fname = 'ac4119_train_set_{0}'.format(i)\n",
    "    positive_examples = train[train['label']==1].sample(frac=.8)\n",
    "    # Unioning the positive and negative examples \n",
    "    # Then shuffling so that not all negative examples are at the end\n",
    "    train_set = pd.concat([negative_examples, positive_examples], ignore_index=True).sample(frac=1)\n",
    "    training_sets.append(train_set)\n",
    "    #train_set.to_csv(base + train_set_fname, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 training sets.\n"
     ]
    }
   ],
   "source": [
    "return_text = \"There are {0} training sets.\".format(len(training_sets))\n",
    "print(return_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.18.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from spacy>=2.2.2->en_core_web_sm==2.2.5) (27.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2018.1.18)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12012547 sha256=f6b1a5323f058e31c0a5a7f96875c002d5ccf8cf9a09bb6771737cd5e5f796d4\n",
      "  Stored in directory: /private/var/folders/r1/8cxq0ypj6gxcdyyxfb6yyjmj051lpw/T/pip-ephem-wheel-cache-2oe8668v/wheels/b5/94/56/596daa677d7e91038cbddfcf32b591d0c915a1b3a3e3d3c79d\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_obj(fname):\n",
    "    directory = '../../data/processed/dev/'\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(directory + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# load_obj(word_freq , 'word_freq_ac4119')\n",
    "word_freq_real = load_obj('word_freq_real_ac4119')\n",
    "word_freq_fake = load_obj('word_freq_fake_ac4119')\n",
    "\n",
    "sorted_freq_fake = {k: v for k, v in sorted(word_freq_fake.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_freq_real = {k: v for k, v in sorted(word_freq_real.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "top_fake_words = list(sorted_freq_fake.keys())[:100]\n",
    "top_real_words = list(sorted_freq_real.keys())[:100]\n",
    "\n",
    "top_overlapping_words = list(set(top_fake_words).intersection(set(top_real_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_overlapping_words = ['', 'around', 'time', 'staff', 'wait', 'went', 'worth', 'made', 've', 'bit', 'pork', 'new', 'm', 'chicken', 'way', 'well', 'fresh', 'good', 'delicious', 'pretty', 'could', 'service', 'friendly', 'amazing', 'come', 'us', 'friend', 'like', 'night', 'go', 'place', 'also', 'bar', 'better', 'always', 'one', 'salad', 'make', 'got', 'food', 'great', 'long', 'never', 'say', 'dinner', 'try', 'two', 'know', 'nice', 'restaurant', 'definitely', 'order', 'people', 'would', 'eat', 'back', 'much', 'ever', 'really', 'sauce', 'table', 'even', 'menu', 'experience', 'ordered', 'perfect', 'want', 're', 'love', 'wine', 'cheese', 'came', 'meal', 'ca', 'nt', 's', 'right', 'small', 'everything', 'get', 'pizza', 'best', 'little', 'think', 'first', 'brunch', 'friends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "parser = English()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def custom_parser(sentence):\n",
    "    pre_token = nlp(sentence)\n",
    "    tokens = [token.text for token in pre_token ]\n",
    "    # removing punctuation\n",
    "\n",
    "    # correcting spelling mistakes \n",
    "    misspelled = spell.unknown(tokens)\n",
    "    stg_tokens_1 = [spell.correction(word) if word in misspelled  else word for word in tokens ]\n",
    "\n",
    "    # removing stop words and top overlapping words\n",
    "    stg_tokens_2 = [ word for word in stg_tokens_1 if word not in stop_words and word not in top_overlapping_words ]\n",
    "    new_sentence = ' '.join(stg_tokens_2)\n",
    "\n",
    "    # retaining lemma\n",
    "    parser = English()\n",
    "    stg_tokens_3 = parser(new_sentence)\n",
    "    stg_tokens_4 = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in stg_tokens_3 ]\n",
    "\n",
    "    # removing punctuation\n",
    "    custom_tokens = [ word for word in stg_tokens_4 if word not in punctuations]\n",
    "\n",
    "    return custom_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"IF YOU ARE A SELF RESPECTING WOMAN WHO IS LOOKING TO EAT SOME DELICIOUS FOOD OR RESERVE A TABLE FOR A BIRTHDAY PARTY LOOK ELSEWHERE PLEASE!!!!!! I made a reservation for a birthday dinner several days before the actual day. They only offered me two different times that were way apart. And by this I mean the interval was ridiculous. Something like 6:45 or 11:30. I opted for the earlier one, even though my friends worked until 7 and could not get there on time. Upon arriving, I took a seat at the bar and was kindly offered a cocktail by the AMAZINGLY helpful and welcoming bartenders. They had taken my card and put it on file to reserve the table, and told me my table would be mine for two hours. While I waited for my friends to arrive at the bar, the MANAGER came up to me and told me I was gorgeous, but he would need to personally take in my dress (since it was a flowing, shirt dress with no belt) because my \"\"dress did not do justice\"\". When I exclaimed, \"\"EXCUSE ME?!\"\" he proceeded to add that I needed to show more leg and my figure because the dress was unflattering. HE CLEARLY HAD A RING ON HIS FINGER AND IS A MARRIED MAN. When my friend interjected and made him know we saw his ring, he simply shrugged and told her she was mad because he didn't flirt with her. :0 Once my friends arrived at about 7:30, I went up to the maitre d and informed him. He looked at me and told me he sat down others in MY TABLE, WHICH WAS RESERVED FOR 2 HOURS. He then continued to add that walk-ins take precedence, which completely baffles me because, JEEZ what is then the point of making a reservation?! IT IS MEANT TO RESERVE A SPOT FOR AN ALLOTTED PERIOD OF TIME. Unfortunately these two massively uncomfortable instances were enough to ruin my perception of the restaurant. The drinks were delicious. The food was as well. But POOR POOR POOR POOR POOR and PUTRID SERVICE. I AM DISGUSTED. The owner would do well in firing their entire management staff. Like I said the food was good, but the quantity was a joke, as well as the pricing for such menial meals. The most surprising thing out of all of this was after I expressed my annoyance I was not even offered an apology. Simple. I will never return again. If you appreciate honesty at its best, you'd believe this review for all its worth, because I give credit where it's due.\"\n",
    "\n",
    "pre_token = nlp(example)\n",
    "tokens = [token.text for token in pre_token ]\n",
    "\n",
    "# correcting spelling mistakes \n",
    "misspelled = spell.unknown(tokens)\n",
    "stg_tokens_1 = [spell.correction(word) if word in misspelled  else word for word in tokens ]\n",
    "\n",
    "# removing stop words and top overlapping words\n",
    "stg_tokens_2 = [ word for word in stg_tokens_1 if word.lower() not in stop_words and word.lower() not in top_overlapping_words ]\n",
    "new_sentence = ' '.join(stg_tokens_2)\n",
    "\n",
    "# retaining lemma\n",
    "parser = English()\n",
    "stg_tokens_3 = parser(new_sentence)\n",
    "stg_tokens_4 = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in stg_tokens_3 ]\n",
    "\n",
    "# removing punctuation\n",
    "custom_tokens = [ word for word in stg_tokens_4 if word not in punctuations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SELF', 'RESPECTING', 'WOMAN', 'LOOKING', 'RESERVE', 'BIRTHDAY', 'PARTY', 'LOOK', '!', '!', '!', '!', '!', '!', 'reservation', 'birthday', 'days', 'actual', 'day', '.', 'offered', 'different', 'times', 'apart', '.', 'mean', 'interval', 'ridiculous', '.', 'p45', '11:30', '.', 'opted', 'earlier', ',', 'worked', '7', '.', 'arriving', ',', 'took', 'seat', 'kindly', 'offered', 'cocktail', 'AMAZINGLY', 'helpful', 'welcoming', 'bartenders', '.', 'taken', 'card', 'file', 'reserve', ',', 'told', 'hours', '.', 'waited', 'arrive', ',', 'MANAGER', 'told', 'gorgeous', ',', 'need', 'personally', 'dress', '(', 'flowing', ',', 'shirt', 'dress', 'belt', ')', 'dress', 'justice', '.', 'exclaimed', ',', 'EXCUSE', '?', '!', 'proceeded', 'add', 'needed', 'leg', 'figure', 'dress', 'unflattering', '.', 'CLEARLY', 'RING', 'FINGER', 'MARRIED', 'MAN', '.', 'interjected', 'saw', 'ring', ',', 'simply', 'shrugged', 'told', 'mad', 'flirt', '.', 'm0', 'arrived', 'ac30', ',', 'maitre', 'd', 'informed', '.', 'looked', 'told', 'sat', ',', 'RESERVED', '2', 'HOURS', '.', 'continued', 'add', 'walk', '-', 'ins', 'precedence', ',', 'completely', 'baffles', ',', 'JEEZ', 'point', 'making', 'reservation', '?', '!', 'MEANT', 'RESERVE', 'SPOT', 'ALLOTTED', 'PERIOD', '.', 'Unfortunately', 'massively', 'uncomfortable', 'instances', 'ruin', 'perception', '.', 'drinks', '.', '.', 'POOR', 'POOR', 'POOR', 'POOR', 'POOR', 'PUTRID', '.', 'DISGUSTED', '.', 'owner', 'firing', 'entire', 'management', '.', 'said', ',', 'quantity', 'joke', ',', 'pricing', 'menial', 'meals', '.', 'surprising', 'thing', 'expressed', 'annoyance', 'offered', 'apology', '.', 'Simple', '.', 'return', '.', 'appreciate', 'honesty', ',', 'd', 'believe', 'review', ',', 'credit', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stg_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.69 s ± 141 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit clean_tokens(word_tokenize(train_set['review'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49 s ± 74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit clean_tokens(word_tokenize(train_set['review'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56 s ± 54.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit clean_tokens(word_tokenize(train_set['review'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set['token_review'] = train_set['review'].map(lambda x: clean_tokens(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_obj(fname):\n",
    "    directory = '../../data/processed/dev/'\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(directory + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Out of the top 100 words for both real and fake reviews {} overlapped\".format(len(top_overlapping_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cnt_vectorizer = CountVectorizer( tokenizer=clean_tokens, binary=True, min_df=5)\n",
    "tfidf_vectorizer = TfidfVectorizer( tokenizer=clean_tokens, binary=True, min_df=5)\n",
    "cnt_vectorizer.fit(train_set['review'])\n",
    "tfidf_vectorizer.fit(train_set['review'])\n",
    "\n",
    "cnt_X_train = cnt_vectorizer.transform(train_set['review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train_set['review'])\n",
    "\n",
    "# cnt_X_dev = cnt_vectorizer.transform(validation['review'])\n",
    "# tfidf_X_dev = tfidf_vectorizer.transform(validation['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5ff3a2411d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tfidf_lr = LogisticRegression()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfitted_tfidf_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# fitted_model =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "params = {'solver':'liblinear', 'max_iter':1000, 'class_weight': 'balanced', 'random_state': 519}\n",
    "tfidf_lr = LogisticRegression(**params )\n",
    "# tfidf_lr = LogisticRegression()\n",
    "fitted_tfidf_lr = tfidf_lr.fit(tfidf_X_train, Y_train)\n",
    "# fitted_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitted_model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
