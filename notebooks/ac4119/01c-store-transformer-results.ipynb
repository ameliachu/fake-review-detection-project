{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = '/Users/chuamelia/Google Drive/Spring 2020/Machine Learning/fake-review-detection-project/data/processed/dev/'\n",
    "\n",
    "def load_obj(fname,  base=base):\n",
    "    # This loads the pickled object.\n",
    "    with open(base + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, fname,  base=base):\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(base + fname + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_tokenizer(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in training set...\n",
      "ac4119_train_set_0_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_1_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_2_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_3_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_4_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_5_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_6_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "Reading in training set...\n",
      "ac4119_train_set_7_w_tokens.csv\n",
      "Convert token_review back to list..\n",
      "Fitting Vectorizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/chuamelia/anaconda/envs/py35/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print('Reading in training set...')\n",
    "    train_fname = 'ac4119_train_set_{0}_w_tokens.csv'.format(i)\n",
    "    train = pd.read_csv(base + train_fname)\n",
    "    print(train_fname)\n",
    "\n",
    "    print('Convert token_review back to list..')\n",
    "    # train['token_review'] = train['token_review'].fillna('[]')\n",
    "    train['token_review'] = train['token_review'].apply(lambda x: literal_eval(x))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, \n",
    "                                       stop_words='english', \n",
    "                                       lowercase=False, binary=True, \n",
    "                                       min_df=5)\n",
    "\n",
    "    cnt_vectorizer = CountVectorizer(tokenizer=identity_tokenizer,  \n",
    "                                     stop_words='english', lowercase=False, \n",
    "                                     binary=True,\n",
    "                                     min_df=5)\n",
    "    print('Fitting Vectorizers...')\n",
    "    cnt_vectorizer.fit(train['token_review'])\n",
    "    tfidf_vectorizer.fit(train['token_review'])\n",
    "\n",
    "    cnt_vectorizer_fname = 'ac4119_X_train_set_{0}_cnt_vectorizer'.format(i)\n",
    "    tfidf_vectorizer_fname = 'ac4119_X_train_set_{0}_tfidf_vectorizer'.format(i)\n",
    "\n",
    "    save_obj(cnt_vectorizer, cnt_vectorizer_fname )\n",
    "    save_obj(tfidf_vectorizer, tfidf_vectorizer_fname)\n",
    "    print('saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejoin positive examples (Train Set #7)\n",
    "For Train Set 7, I tried creating tokens for the negative examples only, the time save was negligible so, I didn't proceed with this methodology on the other training sets.\n",
    "\n",
    "Rejoining Train Set 7 with its tokenized positive training examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_7 = pd.read_csv(base + 'ac4119_train_set_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts7_exids = list(train_set_7[train_set_7['label'] == 1]['ex_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_6 = pd.read_csv(base + 'ac4119_train_set_6_w_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts6_exids = list(train_set_6[train_set_6['label'] == 1]['ex_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_examples = set(ts7_exids).intersection(set(ts6_exids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overlapping_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16545"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if intersection worked as intended\n",
    "len([i for i in ts7_exids if i in overlapping_examples ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts6_append = train_set_6[train_set_6['ex_id'].isin(overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = [ex for ex in ts7_exids if ex  not in overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4110"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_5 = pd.read_csv(base + 'ac4119_train_set_5_w_tokens.csv')\n",
    "ts5_exids = list(train_set_5[train_set_5['label'] == 1]['ex_id'].values)\n",
    "ts5_overlapping_examples = set(remaining_examples).intersection(set(ts5_exids))\n",
    "ts5_append = train_set_5[train_set_5['ex_id'].isin(ts5_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts5_append.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples_2 = [ex for ex in remaining_examples if ex not in ts5_overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2960"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_4 = pd.read_csv(base + 'ac4119_train_set_4_w_tokens.csv')\n",
    "ts4_exids = list(train_set_4[train_set_4['label'] == 1]['ex_id'].values)\n",
    "ts4_overlapping_examples = set(remaining_examples_2).intersection(set(ts4_exids))\n",
    "ts4_append = train_set_4[train_set_4['ex_id'].isin(ts4_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2367"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts4_overlapping_examples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_examples_3 = [ex for ex in remaining_examples_2 if ex not in ts4_overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_1 = pd.read_csv(base + 'ac4119_train_set_1_w_tokens.csv')\n",
    "ts1_exids = list(train_set_1[train_set_1['label'] == 1]['ex_id'].values)\n",
    "ts1_overlapping_examples = set(remaining_examples_3).intersection(set(ts1_exids))\n",
    "ts1_append = train_set_1[train_set_1['ex_id'].isin(ts1_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts1_overlapping_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_examples_4 = [ex for ex in remaining_examples_3 if ex not in ts1_overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_2 = pd.read_csv(base + 'ac4119_train_set_2_w_tokens.csv')\n",
    "ts2_exids = list(train_set_2[train_set_2['label'] == 1]['ex_id'].values)\n",
    "ts2_overlapping_examples = set(remaining_examples_4).intersection(set(ts2_exids))\n",
    "ts2_append = train_set_2[train_set_2['ex_id'].isin(ts2_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts2_overlapping_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_examples_5 = [ex for ex in remaining_examples_4 if ex not in ts2_overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_0 = pd.read_csv(base + 'ac4119_train_set_0_w_tokens.csv')\n",
    "ts0_exids = list(train_set_0[train_set_0['label'] == 1]['ex_id'].values)\n",
    "ts0_overlapping_examples = set(remaining_examples_5).intersection(set(ts0_exids))\n",
    "ts0_append = train_set_0[train_set_0['ex_id'].isin(ts0_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts0_overlapping_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_examples_6 = [ex for ex in remaining_examples_5 if ex not in ts0_overlapping_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_3 = pd.read_csv(base + 'ac4119_train_set_3_w_tokens.csv')\n",
    "ts3_exids = list(train_set_3[train_set_3['label'] == 1]['ex_id'].values)\n",
    "ts3_overlapping_examples = set(remaining_examples_6).intersection(set(ts3_exids))\n",
    "ts3_append = train_set_3[train_set_3['ex_id'].isin(ts3_overlapping_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts3_overlapping_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2724, 106278, 155927, 207889, 234998, 303713}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts3_overlapping_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_example_dfs = [ts0_append, ts1_append, ts2_append, ts4_append, ts5_append, ts6_append, ts3_append]\n",
    "positive_examples = pd.concat(positive_example_dfs, ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20655"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_7_neg_only = pd.read_csv(base + 'ac4119_train_set_7_w_tokens_neg_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_7_neg_only['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_7_w_tokens = pd.concat([positive_examples, train_set_7_neg_only], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>token_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>133023</td>\n",
       "      <td>28363</td>\n",
       "      <td>382</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-12-26</td>\n",
       "      <td>First impressions are almost always misleading...</td>\n",
       "      <td>['impressions', 'misleading', 'terms', 'places...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15620</th>\n",
       "      <td>321159</td>\n",
       "      <td>149533</td>\n",
       "      <td>844</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-02</td>\n",
       "      <td>Second the bad bartender review. It's become a...</td>\n",
       "      <td>['second', 'bad', 'bartender', 'review', 'abho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24803</th>\n",
       "      <td>23217</td>\n",
       "      <td>19627</td>\n",
       "      <td>66</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-05-28</td>\n",
       "      <td>I finally made it to Los Tacos and I have to j...</td>\n",
       "      <td>['finally', 'los', 'tacos', 'jump', 'bandwagon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>278695</td>\n",
       "      <td>136182</td>\n",
       "      <td>724</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-03-09</td>\n",
       "      <td>super yummy food and a great service. this res...</td>\n",
       "      <td>['super', 'yummy', 'favourites', 'city', 'list...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25425</th>\n",
       "      <td>134659</td>\n",
       "      <td>80036</td>\n",
       "      <td>388</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>For vegetarian food this place is not bad.  Go...</td>\n",
       "      <td>['vegetarian', 'bad', 'meatless', 'mondays']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ex_id  user_id  prod_id  rating  label        date  \\\n",
       "635    133023    28363      382     4.0      1  2013-12-26   \n",
       "15620  321159   149533      844     2.0      1  2014-09-02   \n",
       "24803   23217    19627       66     5.0      0  2014-05-28   \n",
       "5568   278695   136182      724     4.0      1  2013-03-09   \n",
       "25425  134659    80036      388     3.0      0  2014-11-11   \n",
       "\n",
       "                                                  review  \\\n",
       "635    First impressions are almost always misleading...   \n",
       "15620  Second the bad bartender review. It's become a...   \n",
       "24803  I finally made it to Los Tacos and I have to j...   \n",
       "5568   super yummy food and a great service. this res...   \n",
       "25425  For vegetarian food this place is not bad.  Go...   \n",
       "\n",
       "                                            token_review  \n",
       "635    ['impressions', 'misleading', 'terms', 'places...  \n",
       "15620  ['second', 'bad', 'bartender', 'review', 'abho...  \n",
       "24803  ['finally', 'los', 'tacos', 'jump', 'bandwagon...  \n",
       "5568   ['super', 'yummy', 'favourites', 'city', 'list...  \n",
       "25425       ['vegetarian', 'bad', 'meatless', 'mondays']  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_7_w_tokens.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_7_w_tokens.to_csv(base + 'ac4119_train_set_7_w_tokens.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
