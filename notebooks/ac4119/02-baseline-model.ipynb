{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../../data/raw/train.csv'\n",
    "val_path = '../../data/raw/dev.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset\n",
    "**Methodology:**\n",
    "\n",
    "1. Separate out the negative examples (dominant class)\n",
    "2. Determine the number of dataframes (`num_splits`) needed to incorporate all negative examples.\n",
    "3. Create a list of dataframes containing the different splits of negative examples.\n",
    "4. Concat the positive and negative examples back together.\n",
    "    - For each new training set, include a 80% random sample of the positive examples to aviod overfitting to the\n",
    "    postive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting frac = 1 to shuffle all the data\n",
    "full_negative_examples = train[train['label']==0].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining the number of positive and negative examples \n",
    "# to determine the number of splits  \n",
    "num_pos_examples = positive_examples.count()[0]\n",
    "num_neg_examples = full_negative_examples.count()[0]\n",
    "\n",
    "num_splits = int(round(num_neg_examples / num_pos_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_train_data = [full_negative_examples[ i * num_pos_examples : min((i + 1) * num_pos_examples, num_neg_examples)] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_sets = []\n",
    "for negative_examples in neg_train_data:\n",
    "    positive_examples = train[train['label']==1].sample(frac=.8)\n",
    "    # Unioning the positive and negative examples \n",
    "    # Then shuffling so that not all negative examples are at the end\n",
    "    train_set = pd.concat([negative_examples, positive_examples], ignore_index=True).sample(frac=1)\n",
    "    training_sets.append(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = training_sets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cnt_vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', binary=True)\n",
    "cnt_vectorizer.fit(train['review'])\n",
    "tfidf_vectorizer.fit(train['review'])\n",
    "\n",
    "cnt_X_train = cnt_vectorizer.transform(train['review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train['review'])\n",
    "\n",
    "cnt_X_dev = cnt_vectorizer.transform(validation['review'])\n",
    "tfidf_X_dev = tfidf_vectorizer.transform(validation['review'])\n",
    "\n",
    "Y_train = train['label']\n",
    "Y_dev = validation['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = '/Users/chuamelia/Google Drive/Spring 2020/Machine Learning/fake-review-detection-project/data/processed/dev/'\n",
    "def load_obj(fname,  base=base):\n",
    "    # This loads the pickled object.\n",
    "    with open(base + fname + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, fname,  base=base):\n",
    "    # This writes out a python object as a pickle.\n",
    "    with open(base + fname + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i, train in enumerate(training_sets):\n",
    "    cnt_train_fname = 'ac4119_cnt_X_train_set_{0}'.format(i)\n",
    "    tfidf_train_fname = 'ac4119_tfidf_X_train_set_{0}'.format(i)\n",
    "    \n",
    "    train_labels_fname = 'ac4119_train_set_{0}_labels.csv'.format(i)\n",
    "    dev_labels_fname = 'ac4119_dev_set_{0}_labels.csv'.format(i)\n",
    "    \n",
    "    cnt_dev_fname = 'ac4119_cnt_X_dev_set_{0}'.format(i)\n",
    "    tfidf_dev_fname = 'ac4119_tfidf_X_dev_set_{0}'.format(i)\n",
    "\n",
    "    cnt_vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', binary=True)\n",
    "    cnt_vectorizer.fit(train['review'])\n",
    "    tfidf_vectorizer.fit(train['review'])\n",
    "\n",
    "    cnt_X_train = cnt_vectorizer.transform(train['review'])\n",
    "    tfidf_X_train = tfidf_vectorizer.transform(train['review'])\n",
    "\n",
    "    cnt_X_dev = cnt_vectorizer.transform(validation['review'])\n",
    "    tfidf_X_dev = tfidf_vectorizer.transform(validation['review'])\n",
    "\n",
    "    Y_train = train['label']\n",
    "    Y_dev = validation['label']\n",
    "    \n",
    "    save_obj(cnt_X_train, cnt_train_fname)\n",
    "    save_obj(cnt_X_dev, cnt_dev_fname)\n",
    "    save_obj(tfidf_X_train, tfidf_train_fname)\n",
    "    save_obj(tfidf_X_dev, tfidf_dev_fname)\n",
    "    Y_train.to_csv(base + train_labels_fname, index=False, sep=',')\n",
    "    Y_dev.to_csv(base + dev_labels_fname, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Cases\n",
    "- Fully Unbalanced\n",
    "- Each of the CV pairs\n",
    "\n",
    "Notes:\n",
    "Failed to converge at default max iter and at 200 when lbfgs specified as solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'solver':'liblinear', 'max_iter':1000, 'class_weight': 'balanced', 'random_state': 519}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lr = LogisticRegression(**params )\n",
    "# tfidf_lr = LogisticRegression()\n",
    "fitted_tfidf_lr = tfidf_lr.fit(tfidf_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_lr = LogisticRegression(**params )\n",
    "fitted_cnt_lr = cnt_lr .fit(cnt_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ClassifierMetrics (X_train, Y_train, X_test, Y_test, fitted_model):\n",
    "    Y_pred = fitted_model.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n",
    "    metrics = {'train_accuracy': fitted_model.score(X_train, Y_train),\n",
    "    'test_accuracy': fitted_model.score(X_test, Y_test),\n",
    "    'test_tpr': tpr,\n",
    "    'test_fpr': fpr,\n",
    "    'test_auc': roc_auc_score(Y_test, Y_pred),\n",
    "    'test_ap': average_precision_score(Y_test, Y_pred)}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_metrics = ClassifierMetrics(tfidf_X_train, Y_train, tfidf_X_dev, Y_dev, fitted_tfidf_lr)\n",
    "cnt_metrics = ClassifierMetrics(cnt_X_train, Y_train, cnt_X_dev, Y_dev, fitted_cnt_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.6027340052341444,\n",
       " 'test_ap': 0.13950157985017012,\n",
       " 'test_auc': 0.6336395744649042,\n",
       " 'test_fpr': array([0.       , 0.4051441, 1.       ]),\n",
       " 'test_tpr': array([0.        , 0.67242325, 1.        ]),\n",
       " 'train_accuracy': 0.8538924890804762}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.6568572860404254,\n",
       " 'test_ap': 0.1503918669377533,\n",
       " 'test_auc': 0.6537919245646655,\n",
       " 'test_fpr': array([0.        , 0.34236133, 1.        ]),\n",
       " 'test_tpr': array([0.        , 0.64994518, 1.        ]),\n",
       " 'train_accuracy': 0.7566302206742984}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
