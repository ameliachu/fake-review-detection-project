{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfake = train_data[train_data['label']== 1]\n",
    "fake = train_data[train_data['label'] == 0]\n",
    "true_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,notfake['review'].count()):\n",
    "    true_word_temp = notfake['review'].iloc[i].split()\n",
    "    true_word_list.append(true_word_temp)\n",
    "from itertools import chain\n",
    "list1 = list(chain.from_iterable(true_word_list))\n",
    "from collections import Counter \n",
    "true_mc = Counter(list1).most_common()\n",
    " \n",
    "fake_word_list= fake['review'].str.split(expand=True).stack()\n",
    "from collections import Counter \n",
    "fake_mc = Counter(fake_word_list).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'with',\n",
       " 'it',\n",
       " 'on',\n",
       " 'had',\n",
       " 'to',\n",
       " 'is',\n",
       " 'this',\n",
       " 'for',\n",
       " 'the',\n",
       " 'we',\n",
       " 'I',\n",
       " 'my',\n",
       " 'a',\n",
       " 'and',\n",
       " 'you',\n",
       " 'in',\n",
       " 'The',\n",
       " 'but',\n",
       " 'that',\n",
       " 'was']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = pd.DataFrame(true_mc)\n",
    "df_false = pd.DataFrame(fake_mc)\n",
    "df_false1 = df_false[0:23]\n",
    "df_true1 = df_true[0:23]\n",
    "df_common = list(set(df_true1.iloc[:,0]) & set(df_false1.iloc[:,0]))\n",
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>923</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-08</td>\n",
       "      <td>The food at snack is a selection of popular Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>924</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-16</td>\n",
       "      <td>This little place in Soho is wonderful. I had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>925</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>ordered lunch for 15 from Snack last Friday.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-28</td>\n",
       "      <td>This is a beautiful quaint little restaurant o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>Snack is great place for a  casual sit down lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250677</th>\n",
       "      <td>358685</td>\n",
       "      <td>161043</td>\n",
       "      <td>921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>This place is amazing, I loved it so much that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250678</th>\n",
       "      <td>358686</td>\n",
       "      <td>161044</td>\n",
       "      <td>921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>I love this place. Drinks are great, atmospher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250679</th>\n",
       "      <td>358688</td>\n",
       "      <td>161046</td>\n",
       "      <td>921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>Bijan is the best bar I have been to in Brookl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250680</th>\n",
       "      <td>358689</td>\n",
       "      <td>161047</td>\n",
       "      <td>921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>Great local lounge spot! After work and regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250793</th>\n",
       "      <td>358849</td>\n",
       "      <td>161111</td>\n",
       "      <td>349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-25</td>\n",
       "      <td>First and last time we try this place.  Pizza ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25819 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ex_id  user_id  prod_id  rating  label        date  \\\n",
       "0            0      923        0     3.0      1  2014-12-08   \n",
       "1            1      924        0     3.0      1  2013-05-16   \n",
       "2            2      925        0     4.0      1  2013-07-01   \n",
       "3            3      926        0     4.0      1  2011-07-28   \n",
       "4            4      927        0     4.0      1  2010-11-01   \n",
       "...        ...      ...      ...     ...    ...         ...   \n",
       "250677  358685   161043      921     5.0      1  2012-07-31   \n",
       "250678  358686   161044      921     5.0      1  2012-06-12   \n",
       "250679  358688   161046      921     5.0      1  2011-12-01   \n",
       "250680  358689   161047      921     5.0      1  2011-11-15   \n",
       "250793  358849   161111      349     1.0      1  2014-03-25   \n",
       "\n",
       "                                                   review  \n",
       "0       The food at snack is a selection of popular Gr...  \n",
       "1       This little place in Soho is wonderful. I had ...  \n",
       "2       ordered lunch for 15 from Snack last Friday.  ...  \n",
       "3       This is a beautiful quaint little restaurant o...  \n",
       "4       Snack is great place for a  casual sit down lu...  \n",
       "...                                                   ...  \n",
       "250677  This place is amazing, I loved it so much that...  \n",
       "250678  I love this place. Drinks are great, atmospher...  \n",
       "250679  Bijan is the best bar I have been to in Brookl...  \n",
       "250680  Great local lounge spot! After work and regula...  \n",
       "250793  First and last time we try this place.  Pizza ...  \n",
       "\n",
       "[25819 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-classification-using-k-nearest-neighbors-46fa8a77acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /Users/robin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/robin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/robin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/robin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_NLC_Classifer():\n",
    "    def __init__(self, k=1, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(1) nearest neighbour algorithm and\n",
    "    # returns the label with closest match. \n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            max_sim = 0\n",
    "            max_index = 0\n",
    "            for j in range(self.x_train.shape[0]):\n",
    "                temp = self.document_similarity(x_test[i], self.x_train[j])\n",
    "                if temp > max_sim:\n",
    "                    max_sim = temp\n",
    "                    max_index = j\n",
    "            y_predict.append(self.y_train[max_index])\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cd737544dc19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0ms1_largest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_synset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m    \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0ms2_synset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's1' is not defined"
     ]
    }
   ],
   "source": [
    " def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "          \"\"\"\n",
    "          Calculate the normalized similarity score of s1 onto s2\n",
    "          For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "          Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "          number of largest similarity values found.\n",
    "\n",
    "          Args:\n",
    "              s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "          Returns:\n",
    "              normalized similarity score of s1 onto s2\n",
    "          \"\"\"\n",
    "s1_largest_scores = []\n",
    "\n",
    "for i, s1_synset in enumerate(s1, 0):\n",
    "    max_score = 0\n",
    "    for s2_synset in s2:\n",
    "        if distance_type == 'path':\n",
    "            score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "        else:\n",
    "            score = s1_synset.wup_similarity(s2_synset)                  \n",
    "        if score != None:\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "\n",
    "        if max_score != 0:\n",
    "            s1_largest_scores.append(max_score)\n",
    "\n",
    "mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "return mean_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cnt_vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', binary=True)\n",
    "cnt_vectorizer.fit(train['review'])\n",
    "tfidf_vectorizer.fit(train['review'])\n",
    "\n",
    "cnt_X_train = cnt_vectorizer.transform(train['review'])\n",
    "tfidf_X_train = tfidf_vectorizer.transform(train['review'])\n",
    "\n",
    "cnt_X_dev = cnt_vectorizer.transform(validation['review'])\n",
    "tfidf_X_dev = tfidf_vectorizer.transform(validation['review'])\n",
    "\n",
    "Y_train = train['label']\n",
    "Y_dev = validation['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(cnt_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(cnt_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_pred = classifier.predict(cnt_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26678  5592]\n",
      " [ 2907   741]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     32270\n",
      "           1       0.12      0.20      0.15      3648\n",
      "\n",
      "    accuracy                           0.76     35918\n",
      "   macro avg       0.51      0.51      0.51     35918\n",
      "weighted avg       0.82      0.76      0.79     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(Y_dev, y_new_pred))\n",
    "print(classification_report(Y_dev, y_new_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier10 = KNeighborsClassifier(n_neighbors=10)\n",
    "classifier10.fit(cnt_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new10_pred = classifier10.predict(cnt_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29869  2401]\n",
      " [ 3291   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91     32270\n",
      "           1       0.13      0.10      0.11      3648\n",
      "\n",
      "    accuracy                           0.84     35918\n",
      "   macro avg       0.52      0.51      0.51     35918\n",
      "weighted avg       0.82      0.84      0.83     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_dev, y_new10_pred))\n",
    "print(classification_report(Y_dev, y_new10_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier15 = KNeighborsClassifier(n_neighbors=15)\n",
    "classifier15.fit(cnt_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new15_pred = classifier15.predict(cnt_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29347  2923]\n",
      " [ 3246   402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90     32270\n",
      "           1       0.12      0.11      0.12      3648\n",
      "\n",
      "    accuracy                           0.83     35918\n",
      "   macro avg       0.51      0.51      0.51     35918\n",
      "weighted avg       0.82      0.83      0.82     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_dev, y_new15_pred))\n",
    "print(classification_report(Y_dev, y_new15_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier20 = KNeighborsClassifier(n_neighbors=20)\n",
    "classifier20.fit(cnt_X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new20_pred = classifier20.predict(cnt_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30589  1681]\n",
      " [ 3418   230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     32270\n",
      "           1       0.12      0.06      0.08      3648\n",
      "\n",
      "    accuracy                           0.86     35918\n",
      "   macro avg       0.51      0.51      0.50     35918\n",
      "weighted avg       0.82      0.86      0.84     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_dev, y_new20_pred))\n",
    "print(classification_report(Y_dev, y_new20_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28628  3642]\n",
      " [ 3156   492]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     32270\n",
      "           1       0.12      0.13      0.13      3648\n",
      "\n",
      "    accuracy                           0.81     35918\n",
      "   macro avg       0.51      0.51      0.51     35918\n",
      "weighted avg       0.82      0.81      0.82     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier11 = KNeighborsClassifier(n_neighbors=11)\n",
    "classifier11.fit(cnt_X_train, Y_train)\n",
    "y_new11_pred = classifier11.predict(cnt_X_dev)\n",
    "print(confusion_matrix(Y_dev, y_new11_pred))\n",
    "print(classification_report(Y_dev, y_new11_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29022  3248]\n",
      " [ 3203   445]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     32270\n",
      "           1       0.12      0.12      0.12      3648\n",
      "\n",
      "    accuracy                           0.82     35918\n",
      "   macro avg       0.51      0.51      0.51     35918\n",
      "weighted avg       0.82      0.82      0.82     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier13 = KNeighborsClassifier(n_neighbors=13)\n",
    "classifier13.fit(cnt_X_train, Y_train)\n",
    "y_new13_pred = classifier13.predict(cnt_X_dev)\n",
    "print(confusion_matrix(Y_dev, y_new13_pred))\n",
    "print(classification_report(Y_dev, y_new13_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30212  2058]\n",
      " [ 3349   299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     32270\n",
      "           1       0.13      0.08      0.10      3648\n",
      "\n",
      "    accuracy                           0.85     35918\n",
      "   macro avg       0.51      0.51      0.51     35918\n",
      "weighted avg       0.82      0.85      0.83     35918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier14 = KNeighborsClassifier(n_neighbors=14)\n",
    "classifier14.fit(cnt_X_train, Y_train)\n",
    "y_new14_pred = classifier14.predict(cnt_X_dev)\n",
    "print(confusion_matrix(Y_dev, y_new14_pred))\n",
    "print(classification_report(Y_dev, y_new14_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from am\n",
    "import json\n",
    "base = '../data/processed/dev/'\n",
    "def writeJsonFile(fname, data,  base=base):\n",
    "    with open(base + fname +'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print('Successfully written to {}'.format(fname))\n",
    "    \n",
    "def readJsonFile(fname, base=base):\n",
    "    with open(base + fname + '.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../../data/processed/knn_unbalanced_results_drh382.csv' does not exist: b'../../data/processed/knn_unbalanced_results_drh382.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8e8e8db82f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mknn_model_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'neighbor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_auc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_ap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/processed/knn_unbalanced_results_drh382.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mknn_model_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../../data/processed/knn_unbalanced_results_drh382.csv' does not exist: b'../../data/processed/knn_unbalanced_results_drh382.csv'"
     ]
    }
   ],
   "source": [
    "knn_model_params = {'neighbor':[], 'train_accuracy':[],'test_accuracy':[],'test_auc':[], 'test_ap':[]}\n",
    "results_path = '../../data/processed/knn_unbalanced_results_drh382.csv'\n",
    "x = pd.read_csv(results_path)\n",
    "knn_model_params = x.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from amelia\n",
    "k = kernels[0]\n",
    "print('Now trying: ' + k + ' kernels...')\n",
    "for c in cs[2:]:\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    svm_model_params['kernel'].append(k)\n",
    "    svm_model_params['C'].append(c)\n",
    "    params = {'kernel':k,'C':c, 'gamma':'scale', 'random_state': 519}\n",
    "    fitted_model = fitted_svm(params)\n",
    "    metrics = ClassifierMetrics(tfidf_X_train, Y_train, tfidf_X_dev, Y_dev, fitted_model)\n",
    "    for m_k in metrics.keys():\n",
    "        svm_model_params[m_k].append(metrics[m_k])\n",
    "    elapsed = t.stop()\n",
    "    svm_pd = pd.DataFrame(svm_model_params)\n",
    "    results_path = '../../data/processed/svm_svc_tfidf_unbalanced_results_ac4119.csv'\n",
    "    svm_pd.to_csv(results_path, sep=',')\n",
    "    print('Elapsed time:',elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
